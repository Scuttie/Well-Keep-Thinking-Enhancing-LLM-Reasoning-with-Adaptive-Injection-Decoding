import pandas as pd
from tqdm import tqdm

def judge_correctness_with_gold(
    llm,
    question: str,
    gold_answer: str,
    answer: str,
    llm_answer: str
) -> str:
    """
    question, gold_answer, answer, llm_answer 네 가지 정보를 GPT에게 주고
    'correct' 또는 'incorrect'로만 판정받습니다.

    - llm: LLM 객체(여기서는 llm.chat.completions.create() 등을 사용)
    - question: 질문
    - gold_answer: 실제 정답(ground truth)
    - answer: 사용자가 기록해둔 'answer' 컬럼값 (gold와 같을 수도 있고 다를 수도 있음)
    - llm_answer: LLM이 최종적으로 낸 답변

    GPT 프롬프트에서 아래와 같은 규칙을 적용하여 'correct' 또는 'incorrect' 문자열만 리턴
    """
    # GPT에게 보낼 대화 히스토리 (messages) 구성
    chat_history = [
        {
            "role": "user",
            "content": f"""
========================================================
GOAL
========================================================
You are given a question, its correct answer (ground truth), and an LLM’s response. Your task is to determine whether the LLM’s final answer matches the correct answer.

========================================================
RETURN FORMAT
========================================================
Respond with either “correct” or “incorrect” only.

========================================================
EVALUATION RULES
========================================================
1. Always judge based on the final answer given by the LLM.
   - If the LLM provides reasoning before arriving at a final answer, ignore intermediate steps and only compare the last stated answer with the ground truth.
   - Example:
     ```
     LLM’s Response: Well, the answer is 24 hours. But how did we get there? I know that the total amount of money he earned is 7 times the amount of money he earned from each customer. So, 7(3) = 21.
     ```
     The final answer here is **21**, so compare it with the ground truth.

2. If the LLM selects an incorrect choice from predefined options generated by itself, mark it incorrect—even if it derives the correct answer in its reasoning.
   - Example:
     ```
     LLM’s Response: (A) 11 (B) 13 (C) 15
     Well, I know that 50 - 40 = 10. So she had 10 eggs.
     The final answer is (A).
     ```
     The correct answer is **10**, but the LLM’s selected choice **(A) 11** is incorrect.

3. Ignore example problems the LLM generates during reasoning.
   - If the LLM solves auxiliary example problems generated by itself before answering the given question, disregard those and judge based only on its answer to the main question.
   - Example:
     ```
     Question: James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total meters does he run a week?
     Answer: 540

     LLM’s Response:
     The distributive property states that...
     #### Example A
     Solve for x.
     3(x+2) = 15 → x = 3
     #### Example B
     2(x-3) = 10 → x = 8
     #### Example C
     4(x+5) = 20 → x = 0

     Concept Problem Revisited
     James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint.
     3 × 3 = 9
     9 × 60 = 540

     James runs 540 meters a week.
     ```
     The final answer is **540**, which should be compared to the ground truth.

4. Handle answer format variations properly.

  - 4.1. If the LLM’s answer matches a choice’s meaning but not its letter label, mark it correct.
  - Example:
    ```
    Question:
    What is likely to satisfy someone's curiosity? (A) hear news (B) read book (C) see favorite show (D) comedy show (E) go somewhere

    Answer: A

    LLM’s Response: The answer is hearing news.
    ```
    Since "hearing news" corresponds to choice (A), this is correct.

  - 4.2. For multiple-choice questions where the LLM outputs only the letter choice, ensure it matches the meaning of the correct answer.
  - Example:
    ```
    Answer: (B) read book
    LLM’s Response: B
    ```
    This is correct since (B) corresponds to "read book".

Question: {question}
Gold Answer: {gold_answer}
User's recorded answer: {answer}
LLM's Response: {llm_answer}

Is the predicted answer correct?
"""
        }
    ]

    response = llm.chat.completions.create(
        model="o1-mini",   
        messages=chat_history,
        temperature=0.7
    )

    final_response = response.choices[0].message.content.strip().lower()
    print("GPT 판단:", final_response)

    if final_response not in ["correct", "incorrect"]:
        return "incorrect"

    return final_response


def add_correctness_column_with_gold(df: pd.DataFrame, llm) -> pd.DataFrame:
    """
    df 의 각 행에 대해:
      - question, gold_answer, answer, llm_answer 를 GPT에게 전달하여
        정오답 판정('correct' 또는 'incorrect') -> 'is_correct' 컬럼 생성

    Parameters
    ----------
    df : pd.DataFrame
        각 행에 question, gold_answer, answer, llm_answer 등이 있다고 가정.
    llm : LLM 객체
        llm.chat.completions.create(...) 형태로 호출할 수 있는 객체
    """

    is_correct_list = []

    for idx, row in tqdm(df.iterrows(), total=len(df)):
        question = row.get('question', "")
        gold_answer = row.get('gold_answer', "")
        answer = row.get('answer', "")
        llm_answer = row.get('llm_answer', "")

        judge_result = judge_correctness_with_gold(
            llm=llm,
            question=question,
            gold_answer=gold_answer,
            answer=answer,
            llm_answer=llm_answer
        )
        is_correct_list.append(judge_result)

    df['is_correct'] = is_correct_list
    return df
